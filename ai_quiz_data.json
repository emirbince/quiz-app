[{"question":"What does intelligence fundamentally include?","choice1":"Creativity, vision, and ethics","choice2":"Language, emotion, and empathy","choice3":"Learning, understanding, reasoning","choice4":"Motor control and balance","choice5":"Memory, DNA, and reflexes","answer":"C"},{"question":"Which field coined the term Artificial Intelligence?","choice1":"MIT 1965 Workshop","choice2":"Dartmouth Conference 1957","choice3":"Stanford Symposium 1969","choice4":"Oxford AI Meet 1950","choice5":"Cambridge Lab 1960","answer":"B"},{"question":"Which definition best aligns with Marvin Minsky’s view of AI?","choice1":"AI is about simulating emotions","choice2":"AI mimics the human brain directly","choice3":"AI studies how to simulate human behavior","choice4":"AI is the scientific study of intelligent mechanisms","choice5":"AI is robotics automation","answer":"D"},{"question":"Which is NOT one of the foundational sciences of AI?","choice1":"Neuroscience","choice2":"Philosophy","choice3":"Genetics","choice4":"Mathematics","choice5":"Linguistics","answer":"C"},{"question":"What distinguishes rational behavior from human behavior?","choice1":"Rational behavior is emotional","choice2":"Humans always behave rationally","choice3":"Human behavior is algorithmic","choice4":"Rational behavior is expected and logical","choice5":"Human behavior is always optimal","answer":"D"},{"question":"Which test determines if a machine behaves like a human?","choice1":"Rorschach Test","choice2":"IQ Test","choice3":"Captcha Test","choice4":"Turing Test","choice5":"Psychometric Test","answer":"D"},{"question":"What does a rational agent primarily aim to do?","choice1":"Imitate human thinking","choice2":"Act emotionally","choice3":"Produce random outputs","choice4":"Achieve best outcomes given inputs","choice5":"Use complex grammar","answer":"D"},{"question":"Which learning type includes labeled data?","choice1":"Unsupervised learning","choice2":"Reinforcement learning","choice3":"Semi-supervised learning","choice4":"Supervised learning","choice5":"Clustering","answer":"D"},{"question":"What is the primary goal of classification?","choice1":"Create regression models","choice2":"Predict continuous values","choice3":"Group data by proximity","choice4":"Assign labels to new data","choice5":"Minimize storage size","answer":"D"},{"question":"What is the role of a test dataset in model evaluation?","choice1":"To generate the model","choice2":"To train the model","choice3":"To visualize the model","choice4":"To test model accuracy","choice5":"To build decision trees","answer":"D"},{"question":"Which algorithm uses distance metrics like Euclidean or Manhattan?","choice1":"Naive Bayes","choice2":"SVM","choice3":"KNN","choice4":"C4.5","choice5":"ID3","answer":"C"},{"question":"What is a key disadvantage of KNN?","choice1":"Too few distance metrics","choice2":"Slow for large datasets","choice3":"Can't handle classification","choice4":"Only works on binary data","choice5":"Ignores similarity","answer":"B"},{"question":"Which method finds the widest margin separating classes?","choice1":"Logistic Regression","choice2":"Naive Bayes","choice3":"SVM","choice4":"KNN","choice5":"Decision Trees","answer":"C"},{"question":"In SVM, what are support vectors?","choice1":"Vectors with zero variance","choice2":"Data points that define the decision boundary","choice3":"Centroids of classes","choice4":"Samples with high entropy","choice5":"Features with correlation","answer":"B"},{"question":"Which kernel is commonly used in nonlinear SVM?","choice1":"Sigmoid","choice2":"Polynomial","choice3":"Linear","choice4":"Radial Basis Function (Gaussian)","choice5":"Entropy Kernel","answer":"D"},{"question":"What is the main assumption of Naive Bayes?","choice1":"Features are correlated","choice2":"Data follows linear trends","choice3":"Features are independent","choice4":"All classes are balanced","choice5":"Outliers dominate predictions","answer":"C"},{"question":"What is the solution to the zero-frequency problem in Naive Bayes?","choice1":"Remove zeroes","choice2":"Ignore classes","choice3":"Add a small k","choice4":"Normalize data","choice5":"Use medoids","answer":"C"},{"question":"What does a confusion matrix NOT include?","choice1":"True Positives","choice2":"True Negatives","choice3":"False Negatives","choice4":"True Rates","choice5":"False Positives","answer":"D"},{"question":"What is recall in classification?","choice1":"TP \/ (TP + FN)","choice2":"TP \/ (TP + FP)","choice3":"TN \/ (TN + FP)","choice4":"FP \/ (FP + FN)","choice5":"(TP+TN) \/ Total","answer":"A"},{"question":"Which algorithm uses Information Gain for splitting?","choice1":"CART","choice2":"C4.5","choice3":"ID3","choice4":"Naive Bayes","choice5":"SVM","answer":"C"},{"question":"Which index measures uncertainty in data?","choice1":"Gain Ratio","choice2":"Precision","choice3":"Recall","choice4":"Entropy","choice5":"AUC","answer":"D"},{"question":"What is the main goal of pruning in decision trees?","choice1":"Increase tree size","choice2":"Overfit training data","choice3":"Enhance overfitting","choice4":"Prevent overfitting","choice5":"Maximize entropy","answer":"D"},{"question":"Which metric helps choose the best feature in C4.5?","choice1":"Information Gain","choice2":"Gain Ratio","choice3":"Gini Index","choice4":"Precision","choice5":"Recall","answer":"B"},{"question":"In unsupervised learning, what is the aim of clustering?","choice1":"Assign class labels","choice2":"Label unseen data","choice3":"Maximize similarity within clusters","choice4":"Reduce dimensions","choice5":"Build regression models","answer":"C"},{"question":"What does the Silhouette Index measure?","choice1":"Noise in the model","choice2":"Distance between features","choice3":"Clustering quality","choice4":"Decision tree accuracy","choice5":"Number of centroids","answer":"C"},{"question":"Which algorithm is sensitive to initial centroid selection?","choice1":"Naive Bayes","choice2":"k-means","choice3":"k-medoids","choice4":"SVM","choice5":"PCA","answer":"B"},{"question":"What’s a key advantage of fuzzy c-means?","choice1":"No parameter tuning","choice2":"Binary class prediction","choice3":"Handles overlapping clusters","choice4":"Strict cluster boundaries","choice5":"Uses decision trees","answer":"C"},{"question":"What is PCA primarily used for?","choice1":"Feature extraction","choice2":"Supervised learning","choice3":"Clustering","choice4":"Model evaluation","choice5":"Tree pruning","answer":"A"},{"question":"Which technique reduces dimension using class labels?","choice1":"PCA","choice2":"Kernel PCA","choice3":"LDA","choice4":"t-SNE","choice5":"Naive Bayes","answer":"C"},{"question":"Which dimensionality reduction method preserves nonlinear structure?","choice1":"LDA","choice2":"CART","choice3":"SVM","choice4":"Kernel PCA","choice5":"ID3","answer":"D"},{"question":"What is an advantage of dimensionality reduction?","choice1":"Increased complexity","choice2":"Higher overfitting","choice3":"Better visualization","choice4":"More storage needed","choice5":"Slower predictions","answer":"C"},{"question":"What is overfitting?","choice1":"Model fits both training and unseen data well","choice2":"Model underfits the data","choice3":"Model memorizes noise from training data","choice4":"Model performs best on test set","choice5":"Model uses pruning excessively","answer":"C"},{"question":"Which is a wrapper method for feature selection?","choice1":"Information Gain","choice2":"Forward Selection","choice3":"PCA","choice4":"Gain Ratio","choice5":"Chi-square Test","answer":"B"},{"question":"What does Gain Ratio attempt to fix from Information Gain?","choice1":"Lack of entropy","choice2":"Favoring attributes with many values","choice3":"Missing values","choice4":"Data imbalance","choice5":"Redundancy","answer":"B"},{"question":"Which test raised philosophical concerns about consciousness?","choice1":"ELIZA","choice2":"Captcha","choice3":"Turing Test","choice4":"IQ Test","choice5":"Fuzzy C-Means","answer":"C"},{"question":"In the ROC curve, a classifier that guesses randomly will produce a curve...","choice1":"Along the top-left border","choice2":"Along the main diagonal","choice3":"Under the X-axis","choice4":"Vertically upward","choice5":"Closer to the Y-axis","answer":"B"},{"question":"In classification with class imbalance, which metric can be most misleading?","choice1":"Precision","choice2":"Recall","choice3":"F1 Score","choice4":"Accuracy","choice5":"AUC","answer":"D"},{"question":"What does a high AUC value imply about a model?","choice1":"Perfect recall","choice2":"Perfect accuracy","choice3":"High false positive rate","choice4":"Good class separation ability","choice5":"Model is underfitting","answer":"D"},{"question":"If an SVM has a high C parameter, it will...","choice1":"Favor simpler models","choice2":"Avoid overfitting","choice3":"Allow more misclassifications","choice4":"Enforce fewer constraints","choice5":"Strongly penalize errors","answer":"E"},{"question":"Which of the following would most likely indicate overfitting in a decision tree?","choice1":"Low accuracy on training and test sets","choice2":"High entropy at root node","choice3":"Low depth of the tree","choice4":"High accuracy on training, low on test","choice5":"Use of gain ratio instead of info gain","answer":"D"},{"question":"Which technique avoids overfitting by evaluating subtrees after full growth?","choice1":"Pre-pruning","choice2":"Post-pruning","choice3":"Entropy maximization","choice4":"Feature selection","choice5":"C4.5 pruning","answer":"B"},{"question":"Why can PCA distort classification boundaries?","choice1":"It increases class overlap","choice2":"It eliminates noise","choice3":"It ignores class labels","choice4":"It emphasizes feature scaling","choice5":"It assumes binary classes","answer":"C"},{"question":"In fuzzy c-means clustering, what does a membership degree of 0.5 imply?","choice1":"Data point belongs equally to two clusters","choice2":"The point is unclustered","choice3":"The model is overfitting","choice4":"High inter-cluster similarity","choice5":"A medoid has been misidentified","answer":"A"},{"question":"In SVM, support vectors...","choice1":"Are only used during testing","choice2":"Have zero effect on decision boundaries","choice3":"Define the hyperplane","choice4":"Are always the closest points to each other","choice5":"Are not relevant to kernel-based SVM","answer":"C"},{"question":"In kernel SVM, what does using a Gaussian kernel effectively do?","choice1":"Transforms linear data into categorical","choice2":"Reduces dimensionality","choice3":"Maps data to infinite-dimensional space","choice4":"Normalizes all inputs","choice5":"Finds maximum entropy clusters","answer":"C"},{"question":"Why might ID3 perform poorly on attributes with many distinct values?","choice1":"It normalizes information gain","choice2":"It skips entropy computation","choice3":"It prefers attributes with fewer classes","choice4":"It overfits by favoring specific splits","choice5":"It uses Gini index","answer":"D"},{"question":"What is the practical limitation of rule sets derived from deep decision trees?","choice1":"They eliminate bias","choice2":"They are hard to visualize","choice3":"They generalize well","choice4":"They require pruning","choice5":"They are less interpretable","answer":"E"},{"question":"Which metric increases when both precision and recall increase?","choice1":"Gini Index","choice2":"Entropy","choice3":"Accuracy","choice4":"AUC","choice5":"F1 Score","answer":"E"},{"question":"In a confusion matrix, if FP is very high and FN is low, what’s most likely true?","choice1":"High recall, low precision","choice2":"Low recall, high precision","choice3":"High F1 score","choice4":"Overfitting","choice5":"Equal class distribution","answer":"A"},{"question":"Which dimensionality reduction technique is best for preserving local structure in nonlinear data?","choice1":"LDA","choice2":"PCA","choice3":"t-SNE","choice4":"Forward selection","choice5":"Gain Ratio","answer":"C"},{"question":"Which feature selection method risks overfitting the most?","choice1":"Chi-square filtering","choice2":"Gain Ratio sorting","choice3":"Wrapper method","choice4":"Information Gain","choice5":"Correlation thresholding","answer":"C"},{"question":"In Naive Bayes, what happens if the conditional probability of a word in a class is zero?","choice1":"Class is ignored","choice2":"Prediction halts","choice3":"That feature is dropped","choice4":"It causes zero total probability","choice5":"It increases posterior","answer":"D"},{"question":"When is the Gini index preferred over entropy?","choice1":"When attributes are binary","choice2":"When data is continuous","choice3":"When computation speed is prioritized","choice4":"When class distribution is uniform","choice5":"When recall matters more than precision","answer":"C"},{"question":"In decision tree splitting, which of the following can lead to underfitting?","choice1":"Using too many leaf nodes","choice2":"Overemphasizing precision","choice3":"Choosing attribute with high gain","choice4":"Early stopping via pre-pruning","choice5":"Recursive splitting","answer":"D"},{"question":"What makes the Chinese Room argument a challenge to strong AI?","choice1":"It disproves Turing Test","choice2":"It denies symbolic reasoning","choice3":"It argues machines cannot truly understand","choice4":"It uses machine learning limits","choice5":"It supports ELIZA’s behavior","answer":"C"},{"question":"Which learning approach best aligns with rational agent behavior under uncertainty?","choice1":"Human-like thinking systems","choice2":"Rule-based expert systems","choice3":"Supervised learning","choice4":"Reinforcement learning","choice5":"Unsupervised learning","answer":"D"},{"question":"In KNN, what is the effect of using a very high 'k' value?","choice1":"Overfitting","choice2":"Random predictions","choice3":"Very small neighborhoods","choice4":"Smoother but biased predictions","choice5":"Underutilization of training set","answer":"D"},{"question":"Which clustering algorithm is least sensitive to outliers?","choice1":"K-means","choice2":"Gaussian Mixture Models","choice3":"K-medoids","choice4":"Hierarchical clustering","choice5":"KNN","answer":"C"},{"question":"Why might Gain Ratio be preferred over Information Gain in decision trees?","choice1":"It eliminates entropy","choice2":"It handles skewed class distributions better","choice3":"It prevents bias toward multi-valued attributes","choice4":"It improves pruning accuracy","choice5":"It favors binary splits","answer":"C"},{"question":"In Naive Bayes, which type handles feature presence\/absence as binary variables?","choice1":"Gaussian","choice2":"Multinomial","choice3":"Bernoulli","choice4":"Poisson","choice5":"Uniform","answer":"C"},{"question":"Which is a common flaw in unsupervised clustering evaluation?","choice1":"Using entropy instead of purity","choice2":"Having labeled ground truth","choice3":"Evaluating on training data","choice4":"No inherent accuracy measure","choice5":"Overfitting the test set","answer":"D"},{"question":"What is the primary drawback of high-dimensional data without reduction?","choice1":"More accurate predictions","choice2":"Better similarity scores","choice3":"Overfitting and increased computational cost","choice4":"Higher recall","choice5":"Balanced feature variance","answer":"C"},{"question":"What is a limitation of kernel PCA?","choice1":"Only works on linear problems","choice2":"Cannot scale data","choice3":"Fails on numeric data","choice4":"Does not reduce dimensionality","choice5":"Requires kernel selection and tuning","answer":"E"}]